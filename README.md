<center><img src="./assets/images/home.png" height="50%"></center>

æœ¬é¡¹ç›®åŸºäº[Chinese-LLaMA-Alpaca **V3.1**](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/v3.1) è¿›è¡Œä½¿ç”¨è¯´æ˜ã€‚[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) å¼€åˆ›äº†åŸºäºLLaMAçš„ä¸­æ–‡æ‰©å……æ”¹è¿›ï¼Œåœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨å¹¶ä½¿ç”¨äº†ä¸­æ–‡æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚

**é¡¹ç›®æ„æˆ**ï¼š
```html
.
â”œâ”€â”€ README.md
â”œâ”€â”€ SHA256.md # LLaMAæ¨¡å‹SHAå€¼å¯¹æ¯”
â”œâ”€â”€ notebooks
â”‚Â Â  â”œâ”€â”€ convert_and_quantize_chinese_alpaca_plus.ipynb
â”‚Â Â  â””â”€â”€ convert_and_quantize_chinese_llama.ipynb
â”œâ”€â”€ requirements.txt # ä¾èµ–æ–‡ä»¶
â””â”€â”€ scripts
    â”œâ”€â”€ chinese_sp.model # ä¸­æ–‡è¯è¡¨æ–‡ä»¶
    â”œâ”€â”€ crawl_prompt.py # 1. é€šè¿‡OpenAIçš„å¤§æ¨¡å‹ï¼ˆå¦‚ChatGPTã€GPT4ç­‰ï¼‰ç”Ÿæˆå¯ç”¨äºå¾®è°ƒçš„æ•°æ®
    â”œâ”€â”€ inference_hf.py # å¯¹å¾®è°ƒè®­ç»ƒäº§ç”Ÿçš„LoRAæ¨¡å‹å’ŒåŸå§‹LLaMAæ¨¡å‹åšæ¨ç†
    â”œâ”€â”€ merge_llama_with_chinese_lora.py
    â”œâ”€â”€ merge_tokenizers.py # 2. è¯è¡¨æ‰©å……
    â””â”€â”€ run_clm_pt_with_peft.py
```

## 1.å‡†å¤‡æ•°æ®

ä¸ç®¡ä½ æ˜¯è¦è¿›è¡Œé¢„è®­ç»ƒè¿˜æ˜¯å¾®è°ƒï¼Œä½ éƒ½éœ€è¦å‡†å¤‡æ•°æ®ï¼Œæ•°æ®å‡†å¤‡çš„ä¸¤ç§æ–¹å¼ï¼š
1. ï¼ˆå…¬å¼€ï¼‰å¦‚æœæ‚¨å¯ä»¥ä½¿ç”¨å…¬å¼€çš„æ ‡å‡†çš„å¯ç”¨äºå¾®è°ƒæˆ–è€…è®­ç»ƒçš„æ•°æ®ï¼Œæ‚¨å¯ä»¥è·³è¿‡æ­¤æ­¥éª¤ï¼›
2. ï¼ˆç”Ÿæˆï¼‰å¦‚æœæ‚¨æ²¡æœ‰åˆé€‚çš„å¾®è°ƒæˆ–è€…è®­ç»ƒæ•°æ®ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`scripts/crawl_prompt.py`ç”Ÿæˆç›¸åº”æ•°æ®ã€‚åŸºæœ¬æ€è·¯ä¸ºä½¿ç”¨ChatGPTæˆ–è€…å…¶å®ƒOpenAIé«˜æ•ˆæ¨¡å‹è¿›è¡Œæ•°æ®ç”Ÿæˆã€‚

## 2.å‡†å¤‡LLaMAæƒé‡

```python
# tokenizer
wget https://agi.gpt4.org/llama/LLaMA/tokenizer.model -O ./tokenizer.model
wget https://agi.gpt4.org/llama/LLaMA/tokenizer_checklist.chk -O ./tokenizer_checklist.chk
# 7B
wget https://agi.gpt4.org/llama/LLaMA/7B/consolidated.00.pth -O ./7B/consolidated.00.pth
wget https://agi.gpt4.org/llama/LLaMA/7B/params.json -O ./7B/params.json
wget https://agi.gpt4.org/llama/LLaMA/7B/checklist.chk -O ./7B/checklist.chk
# 13B
wget https://agi.gpt4.org/llama/LLaMA/13B/consolidated.00.pth -O ./13B/consolidated.00.pth
wget https://agi.gpt4.org/llama/LLaMA/13B/consolidated.01.pth -O ./13B/consolidated.01.pth
wget https://agi.gpt4.org/llama/LLaMA/13B/params.json -O ./13B/params.json
wget https://agi.gpt4.org/llama/LLaMA/13B/checklist.chk -O ./13B/checklist.chk
# 30B
wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.00.pth -O ./30B/consolidated.00.pth
wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.01.pth -O ./30B/consolidated.01.pth
wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.02.pth -O ./30B/consolidated.02.pth
wget https://agi.gpt4.org/llama/LLaMA/30B/consolidated.03.pth -O ./30B/consolidated.03.pth
wget https://agi.gpt4.org/llama/LLaMA/30B/params.json -O ./30B/params.json
wget https://agi.gpt4.org/llama/LLaMA/30B/checklist.chk -O ./30B/checklist.chk
# 65B
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.00.pth -O ./65B/consolidated.00.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.01.pth -O ./65B/consolidated.01.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.02.pth -O ./65B/consolidated.02.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.03.pth -O ./65B/consolidated.03.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.04.pth -O ./65B/consolidated.04.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.05.pth -O ./65B/consolidated.05.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.06.pth -O ./65B/consolidated.06.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/consolidated.07.pth -O ./65B/consolidated.07.pth
wget https://agi.gpt4.org/llama/LLaMA/65B/params.json -O ./65B/params.json
wget https://agi.gpt4.org/llama/LLaMA/65B/checklist.chk -O ./65B/checklist.chk
```

æ‚¨éœ€è¦ä¸‹è½½ä¸åŒå¤§å°å‚æ•°çš„LLaMAæ¨¡å‹æƒé‡ï¼Œå‚æ•°è¶Šå¤§çš„æ¨¡å‹æƒé‡ä½“ç§¯è¶Šå¤§ï¼Œç²¾åº¦ç›¸å¯¹è¾ƒå¥½ï¼Œå¾®è°ƒå’Œè®­ç»ƒæ—¶é—´ä¹Ÿç›¸å¯¹è¾ƒé•¿ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ7Bæˆ–è€…13Bæ¨¡å‹å°†æ˜¯å¤§éƒ¨åˆ†äººçš„é€‰æ‹©ã€‚

åŠ¡å¿…ç¡®è®¤LLaMAåŸºæ¨¡å‹çš„å®Œæ•´æ€§ï¼Œæ£€æŸ¥æ˜¯å¦ä¸[SHA256.md](SHA256.md) æ‰€ç¤ºçš„å€¼ä¸€è‡´ï¼Œå¦åˆ™æ— æ³•è¿›è¡Œåˆå¹¶æ“ä½œã€‚

## 3.è½¬åŒ–ä¸ºHFæ ¼å¼æƒé‡

```python
# å®‰è£…ä¾èµ–åº“
pip install git+https://github.com/huggingface/transformers

# è½¬åŒ–HFæƒé‡
python -m transformers.models.llama.convert_llama_weights_to_hf \
   --input_dir llama-weights \
   --model_size 7B \
   --output_dir llama-hf-weights
  
> python -m transformers.models.llama.convert_llama_weights_to_hf --input_dir ./ --model_size 7B --output_dir ./output/7B-hf
```

å¦‚æœä½ ä¸æƒ³è¦è‡ªå·±æ‰‹åŠ¨è½¬åŒ–ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨åˆ«äººè½¬åŒ–å¥½çš„LLaMA-HFæ¨¡å‹ï¼Œ[pinkmanlove](https://huggingface.co/pinkmanlove) æœ‰åœ¨HuggingFaceæä¾›è½¬åŒ–å¥½çš„LLaMA-HFæƒé‡ï¼Œå¦‚æœå¤±æ•ˆå¯ä»¥åœ¨`HuggingFace-Models`æœç´¢å…¶ä»–äººè½¬åŒ–å¥½çš„ã€‚

## 4.è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹

æ•´ä¸ªè®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š
1. è¯è¡¨æ‰©å……ï¼›
2. é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼‰ï¼›
3. æŒ‡ä»¤å¾®è°ƒï¼›

### 4.1è¯è¡¨æ‰©å……

```python
python scripts/merge_tokenizers.py \
  --llama_tokenizer_dir llama_tokenizer_dir \
  --chinese_sp_model_file chinese_sp_model_file

> python scripts/merge_tokenizers.py --llama_tokenizer_dir output/7B-hf --chinese_sp_model_file scripts/chinese_sp.model
```
å‚æ•°è¯´æ˜ï¼š
- `llama_tokenizer_dir`:æŒ‡å‘å­˜æ”¾åŸç‰ˆLLaMA tokenizerçš„ç›®å½•ï¼›
- `chinese_sp_model_file`:æŒ‡å‘ç”¨sentencepieceè®­ç»ƒçš„ä¸­æ–‡è¯è¡¨æ–‡ä»¶ï¼ˆchinese_sp.modelï¼‰ï¼›

### 4.2é¢„è®­ç»ƒï¼ˆå¯é€‰ï¼‰

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨é€šç”¨ä¸­æ–‡è¯­æ–™åœ¨åŸç‰ˆLLaMAæƒé‡çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥è¿›è¡Œé¢„è®­ç»ƒã€‚è¯¥è¿‡ç¨‹åˆåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

- ç¬¬ä¸€é˜¶æ®µï¼šå†»ç»“transformerå‚æ•°ï¼Œä»…è®­ç»ƒembeddingï¼Œåœ¨å°½é‡ä¸å¹²æ‰°åŸæ¨¡å‹çš„æƒ…å†µä¸‹é€‚é…æ–°å¢çš„ä¸­æ–‡è¯å‘é‡ï¼›
- ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨LoRAæŠ€æœ¯ï¼Œä¸ºæ¨¡å‹æ·»åŠ LoRAæƒé‡ï¼ˆadapterï¼‰ï¼Œè®­ç»ƒembeddingçš„åŒæ—¶ä¹Ÿæ›´æ–°LoRAå‚æ•°ï¼›

é¢„è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µä¸­æ¨¡å‹æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ï¼Œå¦‚æœä¸æ˜¯æœ‰ç‰¹åˆ«å……è£•çš„æ—¶é—´å’Œè®¡ç®—èµ„æºï¼Œå»ºè®®è·³è¿‡è¯¥é˜¶æ®µã€‚é¢„è®­ç»ƒçš„ç¬¬äºŒé˜¶æ®µè®­ç»ƒå¦‚ä¸‹ï¼ˆå•æœºå•å¡ï¼‰ï¼š
```python
########å‚æ•°è®¾ç½®########
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=path/to/hf/llama/dir
chinese_tokenizer_path=path/to/chinese/llama/tokenizer/dir
dataset_dir=path/to/pt/data/dir
data_cache=temp_data_cache_dir
per_device_train_batch_size=1
per_device_eval_batch_size=1
training_steps=100
gradient_accumulation_steps=1
output_dir=output_dir

deepspeed_config_file=ds_zero2_no_offload.json

########å¯åŠ¨å‘½ä»¤########
torchrun --nnodes 1 --nproc_per_node 1 run_clm_pt_with_peft.py \
    --deepspeed ${deepspeed_config_file} \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${chinese_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --fp16 \
    --max_steps ${training_steps} \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --save_steps 500 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --torch_dtype float16 \
    --gradient_checkpointing \
    --ddp_find_unused_parameters False
```
å‚æ•°è¯´æ˜ï¼š
- `--model_name_or_path`: åŸç‰ˆHFæ ¼å¼çš„LLaMAæ¨¡å‹æ‰€åœ¨ç›®å½•ï¼›
- `--tokenizer_name_or_path`: Chinese-LLaMA tokenizeræ‰€åœ¨çš„ç›®å½•ï¼ˆmerge_tokenizers.pyåˆæˆçš„ç»“æœï¼‰ï¼›
- `--dataset_dir`: é¢„è®­ç»ƒæ•°æ®çš„ç›®å½•ï¼Œå¯åŒ…å«å¤šä¸ªä»¥txtç»“å°¾çš„çº¯æ–‡æœ¬æ–‡ä»¶ï¼›
- `--data_cache_dir`: æŒ‡å®šä¸€ä¸ªå­˜æ”¾æ•°æ®ç¼“å­˜æ–‡ä»¶çš„ç›®å½•ï¼›

å¤šæœºå¤šå¡ï¼š
```python
torchrun \
  --nnodes ${num_nodes} \
  --nproc_per_node ${num_gpu_per_node} 
  --node_rank ${node_rank} \
  --master_addr ${master_addr} \
  --master_port ${master_port} \
  run_clm_pt_with_peft.py \
    ...
```

ä¸­æ–‡LLaMAæ¨¡å‹åœ¨åŸç‰ˆçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œä½¿ç”¨äº†ä¸­æ–‡é€šç”¨çº¯æ–‡æœ¬æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒã€‚è¿™é‡Œä½œè€…æä¾›äº†ä¸¤ç§ä¸‹è½½è¿™äº›é¢„è®­ç»ƒæƒé‡çš„æ–¹å¼ï¼Œè€Œä¸éœ€è¦æˆ‘ä»¬è‡ªå·±èŠ±è´¹èµ„æºè®­ç»ƒï¼š

- ï¼ˆ1ï¼‰Google Driveæˆ–è€…ç™¾åº¦ç½‘ç›˜

| æ¨¡å‹åç§°                 | è®­ç»ƒæ•°æ® | é‡æ„æ¨¡å‹ | å¤§å° |                    LoRAä¸‹è½½                   |
| :----------------------- | :------: | :--------------------: | :----------------: | :----------------------------------------------------------: |
| Chinese-LLaMA-7B         | é€šç”¨20G  |      åŸç‰ˆLLaMA-7B      |        770M        | [[ç™¾åº¦ç½‘ç›˜]](https://pan.baidu.com/s/1oORTdpr2TvlkxjpyWtb5Sw?pwd=33hb)</br>[[Google Drive]](https://drive.google.com/file/d/1iQp9T-BHjBjIrFWXq_kIm_cyNmpvv5WN/view?usp=sharing) |
| Chinese-LLaMA-Plus-7B â­ï¸  | é€šç”¨120G |      åŸç‰ˆLLaMA-7B      |        790M        | [[ç™¾åº¦ç½‘ç›˜]](https://pan.baidu.com/s/1zvyX9FN-WSRDdrtMARxxfw?pwd=2gtr)</br>[[Google Drive]](https://drive.google.com/file/d/1N97m3rBj-rp-J1X8rgRfluyomEscfAq0/view?usp=sharing) |
| Chinese-LLaMA-13B        | é€šç”¨20G  |     åŸç‰ˆLLaMA-13B      |         1G         | [[ç™¾åº¦ç½‘ç›˜]](https://pan.baidu.com/s/1BxFhYhDMipW7LwI58cGmQQ?pwd=ef3t)<br/>[[Google Drive]](https://drive.google.com/file/d/12q9EH4mfKRnoKlbkkhzv1xDwWnroo9VS/view?usp=sharing) |
| Chinese-LLaMA-Plus-13B â­ï¸ | é€šç”¨120G |     åŸç‰ˆLLaMA-13B      |         1G         | [[ç™¾åº¦ç½‘ç›˜]](https://pan.baidu.com/s/1VGpNlrLx5zHuNzLOcTG-xw?pwd=8cvd)<br/>[[Google Drive]](https://drive.google.com/file/d/1q0L5Me_1j_9iiRRNfuEFUt3SOjQo3-g3/view?usp=share_link) |

- ï¼ˆ2ï¼‰å¯ä»¥åœ¨ğŸ¤—Model Hubä¸‹è½½ä»¥ä¸Šæ‰€æœ‰æ¨¡å‹ï¼Œå¹¶ä¸”ä½¿ç”¨[transformers](https://github.com/huggingface/transformers)å’Œ[PEFT](https://github.com/huggingface/peft)è°ƒç”¨ä¸­æ–‡LLaMAæ¨¡å‹ã€‚ä»¥ä¸‹æ¨¡å‹è°ƒç”¨åç§°æŒ‡çš„æ˜¯ä½¿ç”¨`.from_pretrained()`ä¸­æŒ‡å®šçš„æ¨¡å‹åç§°ã€‚

| æ¨¡å‹å                  | æ¨¡å‹è°ƒç”¨åç§°                            |                             é“¾æ¥                             |
| ----------------------- | :-------------------------------------- | :----------------------------------------------------------: |
| Chinese-LLaMA-7B        | ziqingyang/chinese-llama-lora-7b        | [Model Hub Link](https://huggingface.co/ziqingyang/chinese-llama-lora-7b) |
| Chinese-LLaMA-Plus-7B   | ziqingyang/chinese-llama-plus-lora-7b   | [Model Hub Link](https://huggingface.co/ziqingyang/chinese-llama-plus-lora-7b) |
| Chinese-LLaMA-13B       | ziqingyang/chinese-llama-lora-13b       | [Model Hub Link](https://huggingface.co/ziqingyang/chinese-llama-lora-13b) |
| Chinese-LLaMA-Plus-13B  | ziqingyang/chinese-llama-plus-lora-13b  | [Model Hub Link](https://huggingface.co/ziqingyang/chinese-llama-plus-lora-13b) |


### 4.3æŒ‡ä»¤å¾®è°ƒ

è®­ç»ƒæ–¹æ¡ˆåŒæ ·é‡‡ç”¨äº†LoRAè¿›è¡Œé«˜æ•ˆç²¾è°ƒï¼Œå¹¶è¿›ä¸€æ­¥å¢åŠ äº†å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚

å•æœºå•å¡ï¼š
```python
########å‚æ•°éƒ¨åˆ†########
lr=1e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=path/to/hf/llama/or/merged/llama/dir/or/model_id
chinese_tokenizer_path=path/to/chinese/llama/tokenizer/dir
dataset_dir=path/to/sft/data/dir
per_device_train_batch_size=1
per_device_eval_batch_size=1
training_steps=100
gradient_accumulation_steps=1
output_dir=output_dir
peft_model=path/to/peft/model/dir
validation_file=validation_file_name

deepspeed_config_file=ds_zero2_no_offload.json

########å¯åŠ¨å‘½ä»¤########
torchrun --nnodes 1 --nproc_per_node 1 run_clm_sft_with_peft.py \
    --deepspeed ${deepspeed_config_file} \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${chinese_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --do_eval \
    --seed $RANDOM \
    --fp16 \
    --max_steps ${training_steps} \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.03 \
    --weight_decay 0 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --evaluation_strategy steps \
    --eval_steps 250 \
    --save_steps 500 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --max_seq_length 512 \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --torch_dtype float16 \
    --validation_file ${validation_file} \
    --peft_path ${peft_model} \
    --gradient_checkpointing \
    --ddp_find_unused_parameters False
```
å‚æ•°è¯´æ˜ï¼š
- `--tokenizer_name_or_path`: Chinese-Alpaca tokenizeræ‰€åœ¨çš„ç›®å½•ï¼ˆmerge_tokenizers.pyåˆæˆçš„ç»“æœï¼‰ï¼›
- `--dataset_dir`: æŒ‡ä»¤ç²¾è°ƒæ•°æ®çš„ç›®å½•ï¼ŒåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä»¥jsonç»“å°¾çš„Stanford Alpacaæ ¼å¼çš„æŒ‡ä»¤ç²¾è°ƒæ•°æ®æ–‡ä»¶ï¼›
- `--validation_file`: ç”¨ä½œéªŒè¯é›†çš„å•ä¸ªæŒ‡ä»¤ç²¾è°ƒæ–‡ä»¶ï¼Œä»¥jsonç»“å°¾ï¼ŒåŒæ ·éµå¾ªStanford Alpacaæ ¼å¼ï¼›

æ‰€è°“Stanford Alpacaæ ¼å¼å³ï¼š
```json
[
  {"instruction" : ...,
   "input" : ...,
   "output" : ...},
  ...
]
```

é…ç½®è¯´æ˜ï¼š

- å¦‚æœæƒ³ç»§ç»­è®­ç»ƒChinese-Alpacaæ¨¡å‹çš„LoRAæƒé‡ï¼š

    - `--model_name_or_path`: åŸç‰ˆHFæ ¼å¼LLaMAæ¨¡å‹ï¼ˆå¦‚æœç»§ç»­è®­ç»ƒéPlus Alpacaæ¨¡å‹ï¼‰æˆ–åˆå¹¶Chinese-LLaMA-Plus-LoRAåçš„Chinese-LLaMAæ¨¡å‹ï¼ˆå¦‚æœç»§ç»­è®­ç»ƒPlusæ¨¡å‹ï¼‰ï¼›
    - `--peft_path`: Chinese-Alpacaçš„LoRAæƒé‡ç›®å½•ï¼›

æ— éœ€æŒ‡å®š`--lora_rank`ã€`--lora_alpha`ã€`--lora_dropout`ã€`--trainable`å’Œ`--modules_to_save`å‚æ•°ã€‚

- å¦‚æœæƒ³åŸºäºä¸­æ–‡Chinese-LLaMAè®­ç»ƒå…¨æ–°çš„æŒ‡ä»¤ç²¾è°ƒLoRAæƒé‡ï¼š

    - `--model_name_or_path`: åˆå¹¶å¯¹åº”Chinese-LLaMA-LoRAåçš„HFæ ¼å¼Chinese-LLaMAæ¨¡å‹ï¼ˆæ— è®ºæ˜¯å¦æ˜¯Plusæ¨¡å‹ï¼‰ï¼›
    - `--peft_path`: å‹¿æä¾›æ­¤å‚æ•°ï¼Œå¹¶ä¸”ä»è„šæœ¬ä¸­åˆ é™¤ --peft_pathï¼›

éœ€æŒ‡å®š`--lora_rank`ã€`--lora_alpha`ã€`--lora_dropout`ã€`--trainable`å’Œ`--modules_to_save`å‚æ•°ã€‚

å¤šæœºå¤šå¡ï¼š
```python
torchrun \
  --nnodes ${num_nodes} \
  --nproc_per_node ${num_gpu_per_node} 
  --node_rank ${node_rank} \
  --master_addr ${master_addr} \
  --master_port ${master_port} \
  run_clm_sft_with_peft.py \
    ...
```

## åˆå¹¶æƒé‡ï¼ˆHFå’ŒLoRAï¼‰

### å•LoRAæƒé‡åˆå¹¶

> é€‚ç”¨äº Chinese-LLaMA, Chinese-LLaMA-Plus, Chinese-Alpaca

```python
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_or_alpaca_lora \
    --output_type [pth|huggingface]
    --output_dir path_to_output_dir 
```

å‚æ•°è¯´æ˜ï¼š
- `--base_model`ï¼šå­˜æ”¾HFæ ¼å¼çš„LLaMAæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•
- `--lora_model`ï¼šä¸­æ–‡LLaMA/Alpaca LoRAè§£å‹åæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¹Ÿå¯ä½¿ç”¨ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°
- `--output_type`: æŒ‡å®šè¾“å‡ºæ ¼å¼ï¼Œå¯ä¸º`pth`æˆ–`huggingface`ã€‚è‹¥ä¸æŒ‡å®šï¼Œé»˜è®¤ä¸º`pth`
- `--output_dir`ï¼šæŒ‡å®šä¿å­˜å…¨é‡æ¨¡å‹æƒé‡çš„ç›®å½•ï¼Œé»˜è®¤ä¸º./
ï¼ˆå¯é€‰ï¼‰--offload_dirï¼šå¯¹äºä½å†…å­˜ç”¨æˆ·éœ€è¦æŒ‡å®šä¸€ä¸ªoffloadç¼“å­˜è·¯å¾„

å…³äº`output_type`çš„æ›´è¿›ä¸€æ­¥è¯´æ˜ï¼š
- `.pth`æ–‡ä»¶å¯ç”¨äºï¼šä½¿ç”¨llama.cppå·¥å…·è¿›è¡Œé‡åŒ–å’Œéƒ¨ç½²ï¼›
- `.bin`æ–‡ä»¶å¯ç”¨äºï¼šä½¿ç”¨Transformersè¿›è¡Œæ¨ç†ï¼›ä½¿ç”¨text-generation-webuiæ­å»ºç•Œé¢ï¼›

### å¤šLoRAæƒé‡åˆå¹¶

> åˆå¹¶Chinese-Alpaca-Pluséœ€è¦æä¾›ä¸¤ä¸ªLoRAæƒé‡ï¼Œåˆ†åˆ«ä¸ºChinese-LLaMA-Plus-LoRAå’ŒChinese-Alpaca-Plus-LoRA

```python
python scripts/merge_llama_with_chinese_lora.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_plus_lora,path_to_chinese_alpaca_plus_lora \
    --output_type [pth|huggingface]
    --output_dir path_to_output_dir 
```

âš ï¸ ä¸¤ä¸ªLoRAæ¨¡å‹çš„é¡ºåºå¾ˆé‡è¦ï¼Œä¸èƒ½é¢ å€’ã€‚å…ˆå†™LLaMA-Plus-LoRAç„¶åå†™Alpaca-Plus-LoRAã€‚ âš ï¸

## è¿è¡Œæ¨¡å‹

```python
CUDA_VISIBLE_DEVICES=0 python scripts/inference_hf.py \
    --base_model path_to_original_llama_hf_dir \
    --lora_model path_to_chinese_llama_or_alpaca_lora \
    --with_prompt \
    --interactive
```

å¦‚æœä¹‹å‰å·²æ‰§è¡Œäº†`merge_llama_with_chinese_lora_to_hf.py`è„šæœ¬å°†loraæƒé‡åˆå¹¶ï¼Œé‚£ä¹ˆæ— éœ€å†æŒ‡å®š`--lora_model`ï¼Œå¯åŠ¨æ–¹å¼æ›´ç®€å•ï¼š
```python
CUDA_VISIBLE_DEVICES=0 python scripts/inference_hf.py \
    --base_model path_to_merged_llama_or_alpaca_hf_dir \
    --with_prompt \
    --interactive
```

## å‚è€ƒ

- https://github.com/ymcui/Chinese-LLaMA-Alpaca
